# Quick Start Guide

Get your Kinic AI Memory Agent on Monad running!

## üéâ Current Status

‚úÖ **Already Deployed:**
- Internet Computer Canister: `2x5sz-ciaaa-aaaak-apgta-cai`
- AI Agent: Claude Haiku integrated
- Backend API: 100% complete and tested
- kinic-cli: Built and working on Windows

‚è≥ **Waiting for:**
- Monad tokens (contacted team)
- Smart contract deployment (when tokens arrive)

## Prerequisites

- Python 3.11+
- Anthropic API key (for Claude AI agent)
- Monad wallet: `0xDA9f4e4C9596a1dA338164FD22895D8C167C6Bd6` (waiting for tokens)

**Optional (for development):**
- Rust nightly (kinic-cli already built on Windows at `C:\kinic-cli`)
- DFX CLI (IC identity already configured)

## Quick Test (Current State)

Since the IC canister and AI agent are already deployed, you can test locally:

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure environment
# The .env already has:
# - KINIC_MEMORY_ID=2x5sz-ciaaa-aaaak-apgta-cai
# - ANTHROPIC_API_KEY=sk-ant-api03-...
# - IC identity credentials
# Missing: MONAD_CONTRACT_ADDRESS (pending deployment)

# 3. Run locally (without Monad logging for now)
uvicorn src.main:app --reload

# 4. Test AI chat with memory
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Tell me about ZKML",
    "top_k": 3
  }'
```

## When Monad Tokens Arrive

Once the Monad team sends tokens to `0xDA9f4e4C9596a1dA338164FD22895D8C167C6Bd6`, complete the deployment:

```bash
# 1. Verify you have tokens
# Check balance at https://monad.xyz or using Web3

# 2. Deploy smart contract
cd /home/hshadab/monad/kinic-monad-poc
source venv/bin/activate
python contracts/deploy.py

# 3. Update .env with contract address
# The deploy script will show: "Contract deployed at: 0x..."
nano .env  # Add the MONAD_CONTRACT_ADDRESS

# 4. Test complete system
./scripts/test_local.sh
```

## Test IC Canister (Already Working!)

From Windows PowerShell where kinic-cli is built:

```powershell
cd C:\kinic-cli

# Insert test memory
.\target\release\kinic-cli.exe --identity kinic_local --ic insert `
  --memory-id 2x5sz-ciaaa-aaaak-apgta-cai `
  --text "Test memory from quickstart" `
  --tag "test,quickstart"

# Search memories
.\target\release\kinic-cli.exe --identity kinic_local --ic search `
  --memory-id 2x5sz-ciaaa-aaaak-apgta-cai `
  --query "test memory"
```

## Deploy to Render (After Monad Contract)

**Note:** Render deployment requires the Monad contract address. Complete this after tokens arrive and contract is deployed.

1. **Push to GitHub**
   ```bash
   git init
   git add .
   git commit -m "Kinic AI Memory Agent on Monad"
   git remote add origin https://github.com/YOUR_USERNAME/kinic-monad-poc
   git push -u origin main
   ```

2. **Create Render Service**
   - Go to https://render.com/dashboard
   - Click "New +" ‚Üí "Web Service"
   - Connect your GitHub repo
   - Render auto-detects Dockerfile
   - Click "Create Web Service"

3. **Set Environment Variables**

   In Render dashboard, add these secrets:

   | Key | Value | Current Status |
   |-----|-------|----------------|
   | `MONAD_CONTRACT_ADDRESS` | `0x...` | ‚è≥ After contract deployment |
   | `MONAD_PRIVATE_KEY` | `0x513c988...` | ‚úÖ Already have |
   | `MONAD_RPC_URL` | `https://rpc-mainnet.monadinfra.com/rpc/...` | ‚úÖ Already have |
   | `KINIC_MEMORY_ID` | `2x5sz-ciaaa-aaaak-apgta-cai` | ‚úÖ Already deployed |
   | `IC_IDENTITY_NAME` | `kinic_local` | ‚úÖ Already configured |
   | `IC_IDENTITY_PEM` | `-----BEGIN EC...` | ‚úÖ Already have (from .env) |
   | `ANTHROPIC_API_KEY` | `sk-ant-api03-...` | ‚úÖ Already have (from .env) |

4. **Deploy!**

   Render will automatically build and deploy your service.

5. **Test Live Endpoint**
   ```bash
   # Health check
   curl https://YOUR-APP.onrender.com/health

   # Chat with AI agent
   curl -X POST https://YOUR-APP.onrender.com/chat \
     -H "Content-Type: application/json" \
     -d '{
       "message": "What do you know about blockchain?",
       "top_k": 3
     }'

   # Insert memory
   curl -X POST https://YOUR-APP.onrender.com/insert \
     -H "Content-Type: application/json" \
     -d '{
       "content": "# Monad\nHigh-performance EVM blockchain with 10,000 TPS",
       "user_tags": "monad,blockchain"
     }'
   ```

## What You Built

üéâ **You now have:**

- ‚úÖ An **AI-powered memory agent** using Claude Haiku
- ‚úÖ Semantic memory storage on **Internet Computer** (via Kinic)
- ‚úÖ Rich, human-readable metadata logged on **Monad blockchain**
- ‚úÖ FastAPI service with **chat, insert, search** endpoints
- ‚úÖ Provable audit trail of all memory operations
- ‚úÖ Context-aware conversations with memory retrieval

**Current Status:**
- ‚úÖ IC Canister: Deployed and tested (`2x5sz-ciaaa-aaaak-apgta-cai`)
- ‚úÖ AI Agent: Integrated and working
- ‚è≥ Monad Contract: Waiting for tokens
- ‚è≥ Render Deployment: Ready when contract is deployed

## View On-Chain Data (After Contract Deployment)

Once the Monad contract is deployed, anyone can query to see what your agent learned:

```python
from web3 import Web3

# Use mainnet RPC
w3 = Web3(Web3.HTTPProvider("https://rpc-mainnet.monadinfra.com/rpc/YOUR_KEY"))
contract = w3.eth.contract(address="YOUR_CONTRACT_ADDRESS", abi=ABI)

# Get latest memory
total = contract.functions.getTotalMemories().call()
memory = contract.functions.getMemory(total - 1).call()

print(f"User: {memory[0]}")       # Who stored it
print(f"Title: {memory[2]}")      # Human-readable title
print(f"Summary: {memory[3]}")    # Content summary
print(f"Tags: {memory[4]}")       # Keywords
print(f"Hash: {memory[5]}")       # Content hash
print(f"Time: {memory[6]}")       # Timestamp
```

## API Examples

### 1. Chat with AI Agent (NEW!)

The AI agent retrieves relevant memories and uses them as context for responses:

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is ZKML and how does it work?",
    "top_k": 3
  }'
```

Response:
```json
{
  "response": "ZKML (Zero-Knowledge Machine Learning) is a framework that enables verifiable ML inference. Jolt Atlas is one implementation that uses zero-knowledge proofs to verify that ML computations were performed correctly without revealing the model or data...",
  "memories_used": [
    {
      "text": "# ZKML Overview\nJolt Atlas enables verifiable ML inference",
      "score": 0.89,
      "tag": "zkml,test"
    }
  ],
  "num_memories": 1,
  "monad_tx": "0xabc123..."
}
```

### 2. Insert Memory

Store new knowledge with automatic metadata extraction:

```bash
curl -X POST http://localhost:8000/insert \
  -H "Content-Type: application/json" \
  -d '{
    "content": "# ZKML\nJolt Atlas enables verifiable ML inference using zero-knowledge proofs",
    "user_tags": "zkml,research"
  }'
```

Response:
```json
{
  "kinic_result": {"status": "inserted"},
  "monad_tx": "0xabc123...",
  "metadata": {
    "title": "ZKML",
    "summary": "Jolt Atlas enables verifiable ML inference using zero-knowledge proofs",
    "tags": "zkml,research,jolt,atlas,enables",
    "content_hash": "0xdef456..."
  }
}
```

### 3. Search Memory

Semantic search (not just keywords):

```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "how does zkml verification work?",
    "top_k": 5
  }'
```

Response:
```json
{
  "results": [
    {
      "score": 0.89,
      "text": "# ZKML\nJolt Atlas enables verifiable ML inference...",
      "tag": "zkml,research"
    }
  ],
  "monad_tx": "0xghi789...",
  "num_results": 1
}
```

### 4. Get Stats

View blockchain statistics:

```bash
curl http://localhost:8000/stats
```

Response:
```json
{
  "total_memories_on_chain": 42,
  "agent_memories": 12,
  "contract_address": "0x...",
  "agent_address": "0xDA9f4e4C9596a1dA338164FD22895D8C167C6Bd6"
}
```

## Troubleshooting

**"kinic-cli binary not found"**

On Windows (where it's already built):
```powershell
cd C:\kinic-cli
# Check if binary exists
ls .\target\release\kinic-cli.exe
```

From WSL/Ubuntu, you can access it:
```bash
/mnt/c/kinic-cli/target/release/kinic-cli.exe --help
```

**"Failed to connect to Monad"**
```bash
# Test mainnet RPC (use your RPC key)
curl https://rpc-mainnet.monadinfra.com/rpc/YOUR_KEY \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}'
```

**"IC identity not found"**

On Windows:
```powershell
# Check identity exists
ls C:\Users\hshad\.config\dfx\identity\kinic_local\identity.pem

# Verify stored in Windows Credential Manager
cd C:\kinic-cli
.\target\release\store_identity.exe kinic_local C:\Users\hshad\.config\dfx\identity\kinic_local\identity.pem
```

**"Anthropic API key invalid"**

Check your `.env` file has:
```bash
ANTHROPIC_API_KEY=sk-ant-api03-...
```

**"Monad contract address not set"**

This is expected! Contract deployment is waiting for MON tokens. The app will work without Monad logging for now (IC storage and AI agent still functional).

## Next Steps

### Immediate:
- [x] IC Canister deployed and tested
- [x] AI Agent integrated (Claude Haiku)
- [x] All backend code complete
- [ ] Wait for Monad tokens from team
- [ ] Deploy Monad smart contract
- [ ] Deploy to Render
- [ ] Build frontend pages (foundation already exists)

### Future Enhancements:
- [ ] Add more content to build knowledge base
- [ ] Multi-user support with wallet authentication
- [ ] Conversation history persistence
- [ ] Analytics dashboard
- [ ] Custom LLM models or local deployment

## Current Architecture

```
User ‚Üí FastAPI (Render) ‚Üí Claude AI + Kinic (IC) + Monad
                           ‚úÖ Working   ‚úÖ Working   ‚è≥ Pending
```

## Need Help?

- üìö **Full documentation**: `README.md`
- üèóÔ∏è **Architecture details**: See README architecture diagram
- üì° **API docs**: See README API endpoints section
- üîß **IC canister**: Already deployed at `2x5sz-ciaaa-aaaak-apgta-cai`
- üí∞ **Monad tokens**: Contact Monad team for address `0xDA9f4e4C9596a1dA338164FD22895D8C167C6Bd6`

---

**Status**: ‚úÖ 95% Complete | ‚è≥ Waiting for Monad Tokens | üöÄ Ready to Deploy

**Last Updated**: November 2025
